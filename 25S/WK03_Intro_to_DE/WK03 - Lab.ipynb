{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832f8bfb",
   "metadata": {},
   "source": [
    "\n",
    "# WK03: **AWS Data Engineering Lab - Part 1**\n",
    "## **Uploading Data to S3 (Separate Folders), Crawling with AWS Glue, and Querying with Athena**\n",
    "\n",
    "### **üìå Objectives:**\n",
    "1. **Manually perform each step** using the **AWS Console**.\n",
    "2. **Automate the process using Boto3** in Python.\n",
    "3. **Upload CSV files to separate folders in S3** to avoid merging tables.\n",
    "4. **Use AWS Glue to create separate tables for each dataset**.\n",
    "5. **Query the data using AWS Athena**.\n",
    "6. **Set appropriate IAM policies for Glue and Athena**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Part 1: Manual Steps Using the AWS Console**\n",
    "\n",
    "### **1Ô∏è‚É£ Upload Data to S3 (AWS Console)**\n",
    "1. Go to the **AWS Management Console** ‚Üí Open **S3**.\n",
    "2. Click **Create Bucket**, set a **globally unique name**, choose a **region**, and click **Create**.\n",
    "3. Click on the newly created **S3 bucket**.\n",
    "4. Click **Create Folder** ‚Üí Name it `population_data/`.\n",
    "5. Click **Create Folder** ‚Üí Name it `economic_data/`.\n",
    "6. Click into each folder and **Upload** the corresponding CSV file:\n",
    "   - **Upload** `population_data.csv` into `population_data/`.\n",
    "   - **Upload** `economic_data.csv` into `economic_data/`.\n",
    "\n",
    "### **2Ô∏è‚É£ Set Up AWS Glue Crawler (AWS Console)**\n",
    "1. Go to **AWS Glue Console** ‚Üí Click **Crawlers** ‚Üí Click **Create Crawler**.\n",
    "2. Name the first crawler **population_crawler**.\n",
    "3. Select **S3** as the source ‚Üí Enter **S3 bucket path**: `s3://your-bucket/datasets/population_data/`.\n",
    "4. Choose **IAM Role** (`AWSGlueServiceRole`) with necessary permissions.\n",
    "5. Select **\"Create a new database\"** ‚Üí Name it (`policy_data_catalog`).\n",
    "6. Click **Create and Run**.\n",
    "7. Repeat the same process for **economic_crawler** with `s3://your-bucket/datasets/economic_data/`.\n",
    "\n",
    "### **3Ô∏è‚É£ Query Data Using AWS Athena (AWS Console)**\n",
    "1. Go to **AWS Athena Console**.\n",
    "2. Set the **Query Results Location** to an **S3 bucket**.\n",
    "3. Select **Database: policy_data_catalog**.\n",
    "4. Run the query:\n",
    "   ```sql\n",
    "   SELECT * FROM population_data LIMIT 10;\n",
    "   ```\n",
    "5. Run a second query:\n",
    "   ```sql\n",
    "   SELECT * FROM economic_data LIMIT 10;\n",
    "   ```\n",
    "6. Review query results.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Part 2: Automating the Process Using Boto3**\n",
    "\n",
    "### **1Ô∏è‚É£ Upload Data to S3 Using Boto3 (Separate Folders)**\n",
    "We will now **automate the S3 upload** by specifying a **directory** and uploading files to separate folders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0015be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Define S3 bucket and directory containing files\n",
    "bucket_name = \"your-bucket-name\"\n",
    "local_directory = \"./data_files/\"  # Change this to your directory path\n",
    "\n",
    "# Define files and their respective folders in S3\n",
    "files_and_folders = {\n",
    "    \"population_data.csv\": \"datasets/population_data/\",\n",
    "    \"economic_data.csv\": \"datasets/economic_data/\"\n",
    "}\n",
    "\n",
    "# Upload files to S3\n",
    "for file, s3_folder in files_and_folders.items():\n",
    "    file_path = os.path.join(local_directory, file)\n",
    "    s3_client.upload_file(file_path, bucket_name, f\"{s3_folder}{file}\")\n",
    "    print(f\"Uploaded {file} to s3://{bucket_name}/{s3_folder}{file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c482138a",
   "metadata": {},
   "source": [
    "\n",
    "### **2Ô∏è‚É£ Set Up AWS Glue Crawlers Using Boto3**\n",
    "The following script will **create a Glue database** and a **Glue Crawler for each dataset** to ensure separate tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "# Define Glue database name\n",
    "glue_database = \"policy_data_catalog\"\n",
    "crawler_population = \"population_crawler\"\n",
    "crawler_economic = \"economic_crawler\"\n",
    "\n",
    "# Create Glue Database\n",
    "glue_client.create_database(DatabaseInput={'Name': glue_database})\n",
    "print(f\"Glue database '{glue_database}' created.\")\n",
    "\n",
    "# Create Glue Crawlers for separate datasets\n",
    "for crawler_name, s3_path in [(crawler_population, \"datasets/population_data/\"), (crawler_economic, \"datasets/economic_data/\")]:\n",
    "    glue_client.create_crawler(\n",
    "        Name=crawler_name,\n",
    "        Role=\"AWSGlueServiceRole\",  # Ensure this IAM role has access to S3 and Glue\n",
    "        DatabaseName=glue_database,\n",
    "        Targets={\"S3Targets\": [{\"Path\": f\"s3://{bucket_name}/{s3_path}\"}]},\n",
    "        TablePrefix=f\"{crawler_name}_\"\n",
    "    )\n",
    "    glue_client.start_crawler(Name=crawler_name)\n",
    "    print(f\"Started Glue Crawler '{crawler_name}'. Wait for it to complete before proceeding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec67c73",
   "metadata": {},
   "source": [
    "\n",
    "### **3Ô∏è‚É£ Query Data Using AWS Athena with Boto3**\n",
    "Now that the data is registered in Glue, we can **query it using Athena**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c38a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "athena_client = boto3.client('athena')\n",
    "\n",
    "# Define Athena queries for both datasets\n",
    "queries = {\n",
    "    \"population_data\": \"SELECT * FROM population_crawler_population_data LIMIT 10;\",\n",
    "    \"economic_data\": \"SELECT * FROM economic_crawler_economic_data LIMIT 10;\"\n",
    "}\n",
    "\n",
    "output_location = f\"s3://{bucket_name}/athena-results/\"\n",
    "\n",
    "# Execute queries\n",
    "for table_name, query in queries.items():\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\"Database\": glue_database},\n",
    "        ResultConfiguration={\"OutputLocation\": output_location}\n",
    "    )\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "    print(f\"Athena Query for {table_name} Execution ID: {query_execution_id}. Results will be in {output_location}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
